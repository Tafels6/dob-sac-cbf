{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.patches as patches\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sac.sac_torch_new import Agent\n",
    "from env.car_2d_with_obs import Car2DWithObstacle\n",
    "from cbf.cbf_random import cbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Car2DWithObstacle()\n",
    "agent = Agent(input_dims=(env.observation_space.shape[0]-1,), env=env,  # ATTENTION: minus 1 for diy env\n",
    "        n_actions=env.action_space.shape[0]) \n",
    "n_games = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disturbance_observer(env, x, x_hat, a):\n",
    "    dt = env.dt\n",
    "    x_delta = x_hat - x\n",
    "    d_hat = -a * x_delta / (np.exp(a * dt) - 1)\n",
    "\n",
    "    return d_hat\n",
    "\n",
    "def state_predictor(env, x, x_hat, u, a):\n",
    "    dt = env.dt\n",
    "    x_delta = x_hat - x\n",
    "    d_hat = -a * x_delta / (np.exp(a * dt) - 1)\n",
    "\n",
    "    px, py, velocity, vx, vy, heading_angle, cos_theta, sin_theta = x\n",
    "    acceleration, front_wheel_angle = u\n",
    "    # beta = np.arctan(0.5 * np.tan(front_wheel_angle))\n",
    "    # angle_velocity = velocity * np.sin(beta)\n",
    "    angle_velocity = 0.5 * velocity * np.tan(front_wheel_angle)\n",
    "\n",
    "    # px, py, v, vx, vy, theta, cos, sin\n",
    "    obs_dt = np.array([velocity*np.cos(heading_angle),\n",
    "                      velocity*np.sin(heading_angle),\n",
    "                      acceleration,\n",
    "                      0,\n",
    "                      0,\n",
    "                      angle_velocity,\n",
    "                      0,\n",
    "                      0])\n",
    "    x_ = x + (obs_dt + d_hat - a * x_delta) * dt\n",
    "    x_[3:5] = np.array([np.cos(x_[-3])*x_[2],np.sin(x_[-3])*x_[2]])\n",
    "    x_[-2:] = np.array([np.cos(x_[-3]),np.sin(x_[-3])])\n",
    "    return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trajectory_1(episode, obs_history, obstacles_history, sense_range, info, writer):\n",
    "    # Create a new figure and axis for plotting\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-3, 8])\n",
    "    ax.set_ylim([-3, 8])\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    plt.xlabel('X Coordinate')\n",
    "    plt.ylabel('Y Coordinate')\n",
    "    plt.title(f'Car Trajectory - Episode {episode}')\n",
    "    plt.grid()\n",
    "\n",
    "    line, = ax.plot([], [], marker='o', markersize=2, linestyle='-')\n",
    "    obstacles_patches = [patches.Circle((obstacle[0], obstacle[1]), obstacle[2], fill=False, color='red') for obstacle in obstacles_history[0]]\n",
    "    for patch in obstacles_patches:\n",
    "        ax.add_patch(patch)\n",
    "    \n",
    "    current_pos, = ax.plot([], [], marker='o', markersize=5, color='green')\n",
    "    car_circle = patches.Circle((0, 0), sense_range, fill=False, color='green', linestyle='--')\n",
    "    ax.add_patch(car_circle)\n",
    "\n",
    "    # Update\n",
    "    def update(frame):\n",
    "        line.set_data([obs[0] for obs in obs_history[:frame+1]], \n",
    "                      [obs[1] for obs in obs_history[:frame+1]])\n",
    "        current_pos.set_data([obs_history[frame][0]], [obs_history[frame][1]])\n",
    "        car_circle.center = obs_history[frame][0], obs_history[frame][1]\n",
    "        for patch, obstacle in zip(obstacles_patches, obstacles_history[frame]):\n",
    "            patch.center = (obstacle[0], obstacle[1])\n",
    "        return [line] + obstacles_patches\n",
    "\n",
    "    anim = FuncAnimation(fig, update, frames=len(obs_history), interval=100, blit=True)\n",
    "    gif_path = f'tmp/plot/epi_{episode}_{info}.gif'\n",
    "    anim.save(gif_path, writer='pillow', fps=30)  # Set frames per second (fps)\n",
    "    plt.close(fig)\n",
    "    # writer.add_figure(f\"Figure/{episode}\", fig, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obs(writer,observation,observation_hat,i,step):\n",
    "    writer.add_scalar(f\"Observation/Episode_{i}/Step/x\", observation[0], step)\n",
    "    writer.add_scalar(f\"Observation/Episode_{i}/Step/y\", observation[1], step)\n",
    "    writer.add_scalar(f\"Observation/Episode_{i}/Step/v\", observation[2], step)\n",
    "    writer.add_scalar(f\"Observation/Episode_{i}/Step/theta\", observation[5], step)\n",
    "    writer.add_scalar(f\"Observation_hat/Episode_{i}/Step/x\", observation_hat[0], step)\n",
    "    writer.add_scalar(f\"Observation_hat/Episode_{i}/Step/y\", observation_hat[1], step)\n",
    "    writer.add_scalar(f\"Observation_hat/Episode_{i}/Step/v\", observation_hat[2], step)\n",
    "    writer.add_scalar(f\"Observation_hat/Episode_{i}/Step/theta\", observation_hat[5], step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('tmp/runs')\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "global_step = 0\n",
    "env.set_seed(4)\n",
    "\n",
    "sense_range = 2.5\n",
    "use_DOB = False\n",
    "use_CBF = True\n",
    "\n",
    "for i in range(n_games):\n",
    "    observation,_ = env.reset()  # px, py, v, vx, vy, theta, cos, sin\n",
    "    modified_observation = np.delete(observation, -3)  # delete theta\n",
    "    observation_hat = np.array(observation)\n",
    "    modified_observation_hat = np.delete(observation_hat, -3)\n",
    "    dob_param = 1e-2\n",
    "    done = False\n",
    "    score = 0\n",
    "    info = {}\n",
    "    step = 0\n",
    "    reward = 0\n",
    "    observation_history = []\n",
    "    obstacles_history = []\n",
    "    observation_history.append(np.array(observation))\n",
    "    obstacles_history.append(np.array(env.obstacles))\n",
    "    last_action = np.zeros(env.action_space.shape[0])\n",
    "    action_max = np.zeros(env.action_space.shape[0])\n",
    "    while not done and step < 500:\n",
    "        save_obs(writer,observation,observation_hat,i,step)\n",
    "    \n",
    "        action_rl = agent.choose_action(modified_observation_hat)\n",
    "        if use_CBF:\n",
    "            d_hat = disturbance_observer(env, observation, observation_hat, dob_param)\n",
    "            action = np.array(cbf(env, observation, action_rl, d_hat,sense_range))\n",
    "        else:\n",
    "            action = action_rl\n",
    "        \n",
    "        observation_hat = state_predictor(env, observation, observation_hat, action, dob_param)\n",
    "        observation_, reward, done,_, info = env.step(action,last_action)\n",
    "        # env.render()\n",
    "\n",
    "        modified_observation_ = np.delete(observation_, -3)\n",
    "        modified_observation_hat_ = np.delete(observation_hat, -3)\n",
    "        score += reward\n",
    "        \n",
    "        if use_DOB:\n",
    "            agent.remember(modified_observation_hat, action, reward, modified_observation_hat_, done)\n",
    "        else:\n",
    "            agent.remember(modified_observation, action, reward, modified_observation_, done)\n",
    "        \n",
    "        # print('observation', observation, 'modified_observation_hat', modified_observation_hat,'modified_observation_hat_', modified_observation_hat_)\n",
    "        \n",
    "        agent.learn(i, step, global_step, writer)\n",
    "        observation = observation_\n",
    "        modified_observation = modified_observation_\n",
    "        modified_observation_hat = modified_observation_hat_\n",
    "        last_action = action\n",
    "        # print('observation: ',observation)\n",
    "        observation_history.append(np.array(observation))\n",
    "        obstacles_history.append(np.array(env.obstacles))\n",
    "        step += 1\n",
    "        global_step += 1\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    writer.add_scalar('avg_score/Episode', avg_score, i)\n",
    "    \n",
    "    np.set_printoptions(formatter={'float': '{:0.2f}'.format})\n",
    "    print('episode', i, ', after', step, 'steps: ', info, ', last state: ', observation, ', with reward: ', np.array([reward]))\n",
    "    if info == {'goal_reached'}:\n",
    "    # if info == {'obstacle_collision'} or (np.all((observation[:2]-5 > -1.5) & (observation[:2]-5 < 1.5))):\n",
    "    # if info == {'obstacle_collision'} or info == {'goal_reached'}:\n",
    "        save_trajectory_1(i,observation_history,obstacles_history,sense_range,info,writer)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
